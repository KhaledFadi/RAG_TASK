# -*- coding: utf-8 -*-
"""RAG_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HoXPu3pnPdneg7jD_x9iSmrGzfxozUhW
"""

!pip install sentence-transformers faiss-cpu rank-bm25 fastapi uvicorn httpx rapidfuzz sacrebleu rouge-score sqlalchemy

import pandas as pd
import re
import json

df1 = pd.read_csv('/content/Natural-Questions-Base.csv')
df2 = pd.read_csv('/content/Natural-Questions-Filtered.csv')
df = pd.concat([df1, df2], ignore_index=True).drop_duplicates()

print("Dataset shape:", df.shape)

def clean(text):
    if not isinstance(text, str):
        return ""
    text = text.replace("\n", " ").replace("\r", " ")
    return re.sub(r"\s+", " ", text).strip()


def chunk(text, max_words=160, overlap=30):
    words = text.split()
    chunks = []
    step = max_words - overlap

    for i in range(0, len(words), step):
        chunks.append(" ".join(words[i:i+max_words]))

    return chunks

def question_type(q):
    m = re.match(r"(who|what|when|where|why|how)", q.lower())
    return m.group(1) if m else "other"


def domain(q):
    q = q.lower()
    if any(x in q for x in ["movie", "actor", "film"]):
        return "entertainment"
    if any(x in q for x in ["city", "country", "capital"]):
        return "geography"
    if any(x in q for x in ["war", "president", "empire"]):
        return "history"
    return "general"


def difficulty(short_answers, long_answer):
    if short_answers and len(long_answer) < 300:
        return "easy"
    if short_answers and len(long_answer) >= 300:
        return "medium"
    return "hard"

records = []

for i, row in df.iterrows():
    q = clean(row.get("question", ""))
    la = clean(row.get("long_answers", ""))
    sa = clean(row.get("short_answers", ""))

    sa_list = [x.strip() for x in re.split(r"[;,|]", sa) if x.strip()]

    rec = {
        "id": f"nq_{i}",
        "question": q,
        "short_answers": sa_list,
        "chunks": chunk(la),
        "metadata": {
            "type": question_type(q),
            "domain": domain(q),
            "difficulty": difficulty(sa_list, la)
        }
    }

    records.append(rec)

print("Processed records:", len(records))

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

model = SentenceTransformer("distiluse-base-multilingual-cased-v2")

texts = []
meta = []

for r in records:
    for c in r["chunks"]:
        texts.append(c)
        meta.append({
            "id": r["id"],
            "question": r["question"],
            "chunk": c,
            "metadata": r["metadata"]
        })

print("Embedding", len(texts), "chunks...")
vectors = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)

dim = vectors.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(vectors.astype("float32"))

print("FAISS ready.")

from rank_bm25 import BM25Okapi

tokenized = [re.findall(r"\w+", t.lower()) for t in texts]
bm25 = BM25Okapi(tokenized)

def normalize_query(q):
    q = q.lower()
    return re.sub(r"\s+", " ", q).strip()

def expand_query(q):
    expansions = {
        "usa": "united states america",
        "ai": "artificial intelligence"
    }
    for k, v in expansions.items():
        if k in q:
            q += " " + v
    return q

def retrieve(query, top_k=5):
    query = normalize_query(query)
    query = expand_query(query)

    qv = model.encode([query], normalize_embeddings=True)
    scores, idxs = index.search(qv.astype("float32"), top_k)

    tokens = re.findall(r"\w+", query.lower())
    bm25_scores = bm25.get_scores(tokens)
    max_bm = max(bm25_scores) if max(bm25_scores) > 0 else 1

    combined = []
    for idx, score in zip(idxs[0], scores[0]):
        hybrid_score = 0.7 * score + 0.3 * (bm25_scores[idx] / max_bm)
        combined.append((idx, hybrid_score))

    combined.sort(key=lambda x: x[1], reverse=True)

    return [meta[i] for i, _ in combined[:top_k]]

import requests
import os

def generate_answer(question, contexts):
    context_text = "\n\n".join([c["chunk"] for c in contexts])

    prompt = f"""
    Answer strictly using the context below.

    Context:
    {context_text}

    Question:
    {question}
    """

    response = requests.post(
        "https://api.groq.com/v1/chat/completions",
        headers={"Authorization": f"Bearer {os.getenv('GROQ_API_KEY')}"},
        json={
            "model": "llama3-70b-8192",
            "messages": [{"role": "user", "content": prompt}]
        }
    )

    return response.json()

from functools import lru_cache

@lru_cache(maxsize=500)
def cached_retrieve(query):
    return retrieve(query)

from rapidfuzz import fuzz
import numpy as np
import sacrebleu
from rouge_score import rouge_scorer


def precision_at_k(contexts, gold, k=5):
    if not gold:
        return 0

    hits = 0
    for c in contexts[:k]:
        for g in gold:
            if fuzz.partial_ratio(c["chunk"].lower(), g.lower()) > 80:
                hits += 1
                break

    return hits / k


def bleu_rouge(hypothesis, gold):
    if not gold:
        return None

    bleu = sacrebleu.corpus_bleu([hypothesis], [gold]).score
    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
    rougeL = max(scorer.score(hypothesis, g)["rougeL"].fmeasure for g in gold)

    return {"BLEU": bleu, "ROUGE-L": rougeL}

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class QuestionRequest(BaseModel):
    question: str
    top_k: int = 5


@app.post("/ask-question")
def ask_question(req: QuestionRequest):
    docs = retrieve(req.question, req.top_k)
    answer = generate_answer(req.question, docs)
    return {"answer": answer}


@app.get("/health")
def health():
    return {"status": "ok"}

# Commented out IPython magic to ensure Python compatibility.
# %%writefile main.py
# 
# from fastapi import FastAPI
# 
# app = FastAPI()
# 
# @app.get("/")
# def root():
#     return {"message": "API working"}

!pip install pyngrok

public_url = ngrok.connect(8000)
print("ðŸš€ Public FastAPI URL:", public_url)
!uvicorn main:app --host 0.0.0.0 --port 8000 --reload --log-level info

from pyngrok import ngrok

ngrok.set_auth_token("32lBaLkWSAbaa3pN0QOLq7UH0xe_uoJ6vrzVhwp794o59qM2")

public_url = ngrok.connect(8000)
print(public_url)

!uvicorn main:app --reload

from fastapi import FastAPI

app = FastAPI()

@app.get("/predict")
def predict(text: str):
    return {"received_text": text}

query = "Who is Nikola Tesla?"
docs = retrieve(query, top_k=3)

for d in docs:
    print("\n--- Retrieved Chunk ---")
    print(d["chunk"])